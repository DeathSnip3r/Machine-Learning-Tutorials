{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in lectures, being able to sample/generate our own data is a useful skill which allows us to experiment with our algorithms on controlled data. Thus, for this week's lab we will be generating our\n",
    "own data to work with. \n",
    "\n",
    "(a) Lets begin by obtaining the data.\n",
    "\n",
    "i. Sample 150 x-values from a Normal distribution using a mean of 0 and standard deviation of 10.\n",
    "Hint: np.random.normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#constants\n",
    "number_samples = 150\n",
    "mean = 0\n",
    "standard_dev = 10\n",
    "\n",
    "#x values random sample generation\n",
    "x_samples = np.random.normal(mean, standard_dev, number_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. From the x-values construct a design matrix using the features {1,x,x^2}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designMatrix_X = np.column_stack((np.ones_like(x_samples), x_samples, x_samples**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Sample true values for theta_0, theta_1 and theta_2 using a uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thTrue = np.random.uniform(-1, 1, size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv. Use your design matrix and the true parameters you obtained to create the y-values for the\n",
    "regression data. Finally add random noise to the y-values using a Normal distribution with mean\n",
    "0 and standard deviation of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanNoise = 0\n",
    "standard_dev_noise = 8\n",
    "yValues = np.dot(designMatrix_X, thTrue) + np.random.normal(meanNoise, standard_dev_noise, size=x_samples.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the x-values and their corresponding y-values on a 2D-axis. Your data should look similar\n",
    "to the data shown in Figure 2a. Hint: pyplot.scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_samples, yValues)\n",
    "plt.axvline(x=0, color='black', linestyle='--')\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "plt.xlabel('x-values')\n",
    "plt.ylabel('y-values')\n",
    "plt.title('Generated Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vi. Split the data into training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(designMatrix_X, yValues, test_size=0.2, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"Number of training examples: {x_train.shape[0]}\")\n",
    "print(f\"Number of validation examples: {x_val.shape[0]}\")\n",
    "print(f\"Number of test examples: {x_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Now that we have data we can train our models.\n",
    "\n",
    "i. Use the Moore-Penrose pseudo-inverse to calculate the closed form solution for the model's parameter values.\n",
    "\n",
    "ii. How close are the learned parameter values to the true parameter values we used to generate the\n",
    "data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed = np.dot(np.linalg.pinv(x_train), y_train)\n",
    "print(\"Learned Parameters:\", closed)\n",
    "print(\"True Parameters:\", thTrue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Compute the training error and validation error for the learned regression model.\n",
    "\n",
    "iv. Create a scatter plot of the individual data points along with the learned regression function,\n",
    "your plot should look like Figure 2b. Hint: pyplot.plot, this plotting function will give weird\n",
    "results if the x-values of the data are not sorted. x train[x train[:,1].argsort()] will give you the\n",
    "design matrix for your training data sorted by the second column (where the x values should be)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingError = np.mean((np.dot(x_train, closed)-y_train)**2)\n",
    "validationError = np.mean((np.dot(x_val, closed)-y_val)**2)\n",
    "print(\"Training Error:\", trainingError)\n",
    "print(\"Validation Error:\", validationError)\n",
    "sorted = x_train[x_train[:,1].argsort()]\n",
    "plt.scatter(x_samples, yValues, label=\"Data Points\")\n",
    "plt.axvline(x=0, color='black', linestyle='--')\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "plt.plot(sorted[:,1], np.dot(sorted, closed), 'r', label=\"Learned Regression Model\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Regression Model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v. Repeat the above process using Gradient Descent to train your model. In addition, plot the\n",
    "training error of your regression model over time (observe or capture the training error every 20\n",
    "parameter updates/time steps). Your plot should look like Figure 2c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(matrixX, yTrained, theta, learnRate, numberIterations):\n",
    "    thetaI = theta\n",
    "    m = matrixX.shape[0]\n",
    "    trainErrors = []\n",
    "    for i in range(numberIterations):\n",
    "        thetaI = thetaI - (learnRate * ((np.dot(matrixX.T, (np.dot(matrixX, thetaI)) - yTrained)))/m)\n",
    "        trainError = np.mean(((np.dot(matrixX, thetaI)) - yTrained)**2)\n",
    "        trainErrors.append(trainError)\n",
    "        if i % 20 == 0:\n",
    "            print(f\"Iteration {i}: Training Error = {trainError}\")\n",
    "    return thetaI, trainErrors\n",
    "\n",
    "thetaI = np.zeros(3)\n",
    "learnRate = 0.000001\n",
    "numberIterations = 100\n",
    "thetaGradientDescent, trainErrors = gradientDescent(x_train, y_train, thetaI, learnRate, numberIterations)\n",
    "\n",
    "plt.plot(np.arange(len(trainErrors)), trainErrors)\n",
    "plt.axvline(x=0, color='black', linestyle='--')\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Training Error')\n",
    "plt.title('Error over Time')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) We will now experiment with overfitting and regularization.\n",
    "\n",
    "i. Begin by appending a third feature to your design matrix for x^3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain_3 = np.hstack((np.ones((x_train.shape[0], 1)), x_train, np.square(x_train[:, 1]).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Train a model using Gradient Descent with the new design matrix. Repeat the process used above\n",
    "in Question 4b. Note, we are now using a third-order polynomial to fit data which was generated\n",
    "using a second-order polynomial. Our function is, thus, more complicated than is necessary to\n",
    "fit the data and as a result will overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaI = np.random.uniform(low=-1, high=1, size=xTrain_3.shape[1])\n",
    "learnRate = 0.000001\n",
    "numberIterations = 100\n",
    "thetaGradientDescent_3, trainErrors_3 = gradientDescent(xTrain_3, y_train, thetaI, learnRate, numberIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Repeat the training process one final time, this time use regularization when training the third-\n",
    "order polynomial model.\n",
    "\n",
    "iv. Compare your results of the three gradient descent based models, which model achives the best\n",
    "final training error? Which model trains the fastest? Which model achieves the best validation\n",
    "error? Can you see any visible difference in the function approximation (fit of the data) by the\n",
    "models or in the learned parameter values? Did any of these models achieve a lower training or\n",
    "validation error than the closed form solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentWithRegularization(matrixX, yTrained, theta, learnRate, numberIterations, lambdaReg):\n",
    "    thetaI = theta\n",
    "    m = matrixX.shape[0]\n",
    "    trainErrors = []\n",
    "    for i in range(numberIterations):\n",
    "        thetaI = thetaI - (learnRate * ((np.dot(matrixX.T, (np.dot(matrixX, thetaI)) - yTrained)) + lambdaReg * thetaI)/m)\n",
    "        trainError = np.mean(((np.dot(matrixX, thetaI)) - yTrained)**2)\n",
    "        trainErrors.append(trainError)\n",
    "        if i % 20 == 0:\n",
    "            print(f\"Iteration {i}: Training Error = {trainError}\")\n",
    "    return thetaI, trainErrors\n",
    "thetaI = np.random.uniform(low=-1, high=1, size=xTrain_3.shape[1])\n",
    "learnRate = 0.000001\n",
    "numberIterations = 100\n",
    "lambdaReg = 0.001\n",
    "thetaGradientDescentReg, trainErrorsReg = gradientDescentWithRegularization(xTrain_3, y_train, thetaI, learnRate, numberIterations, lambdaReg)\n",
    "\n",
    "plt.plot(np.arange(len(trainErrors)), trainErrors, label='Second-order polynomial')\n",
    "plt.plot(np.arange(len(trainErrors_3)), trainErrors_3, label='Third-order polynomial')\n",
    "plt.plot(np.arange(len(trainErrorsReg)), trainErrorsReg, label='Third-order polynomial with Regularization')\n",
    "plt.axvline(x=0, color='black', linestyle='--')\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Training Error')\n",
    "plt.title('Error over Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"Second-order polynomial training error: {trainErrors[-1]}\")\n",
    "print(f\"Third-order polynomial training error: {trainErrors_3[-1]}\")\n",
    "print(f\"Third-order polynomial with regularization training error: {trainErrorsReg[-1]}\")\n",
    "\n",
    "x_plot = np.linspace(-5, 5, 100)\n",
    "y_plot = thetaGradientDescent[0] + thetaGradientDescent[1]*x_plot + thetaGradientDescent[2]*x_plot**2\n",
    "plt.plot(x_plot, y_plot, label='Second-order polynomial')\n",
    "y_plot_3 = thetaGradientDescent_3[0] + thetaGradientDescent_3[1]*x_plot + thetaGradientDescent_3[2]*x_plot**2 + thetaGradientDescent_3[3]*x_plot**3\n",
    "plt.plot(x_plot, y_plot_3, label='Third-order polynomial')\n",
    "y_plot_reg = thetaGradientDescentReg[0] + thetaGradientDescentReg[1]*x_plot + thetaGradientDescentReg[2]*x_plot**2 + thetaGradientDescentReg[3]*x_plot**3\n",
    "plt.plot(x_plot, y_plot_reg, label='Third-order polynomial with regularization')\n",
    "\n",
    "# sort_idx = x_test.argsort(axis=0)\n",
    "# x_test_sorted = x_test[sort_idx]\n",
    "# y_test_sorted = y_test[sort_idx]\n",
    "# plt.scatter(x_test_sorted, y_test_sorted, label='Test data')\n",
    "# plt.xlabel('x')\n",
    "# plt.ylabel('y')\n",
    "# plt.title('Fitted function')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the final test error for all four of your models. Which model obtains the lowest test\n",
    "error? Did the regularisation improve the test error performance of the third-order polynomial\n",
    "model? Which of the three gradient descent models achieved the lowest test error? Would you\n",
    "say it is better to use higher-order polynomials and regularize or use a model which only uses the\n",
    "necessary features to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testError = np.mean((np.dot(x_test, closed)-y_test)**2)\n",
    "print(\"Test Error (Second Order Polynomail):\", testError)\n",
    "xTest_3 = np.hstack((np.ones((x_test.shape[0], 1)), x_test, np.square(x_test[:, 1]).reshape(-1, 1)))\n",
    "testError_3 = np.mean((np.dot(xTest_3, thetaGradientDescent_3)-y_test)**2)\n",
    "print(\"Test Error (Third Error Polynomial):\", testError_3)\n",
    "testErrorReg = np.mean((np.dot(xTest_3, thetaGradientDescentReg)-y_test)**2)\n",
    "print(\"Test Error (Third Error Polynomial with regularization):\", testErrorReg)\n",
    "\n",
    "# # Fit the first-order polynomial model\n",
    "# model1.fit(x_train, y_train)\n",
    "# y_pred1 = model1.predict(x_test)\n",
    "# mse1 = mean_squared_error(y_test, y_pred1)\n",
    "\n",
    "# # Fit the second-order polynomial model\n",
    "# model2.fit(x_train, y_train)\n",
    "# y_pred2 = model2.predict(x_test)\n",
    "# mse2 = mean_squared_error(y_test, y_pred2)\n",
    "\n",
    "# # Fit the third-order polynomial model\n",
    "# model3.fit(x_train, y_train)\n",
    "# y_pred3 = model3.predict(x_test)\n",
    "# mse3 = mean_squared_error(y_test, y_pred3)\n",
    "\n",
    "# # Fit the third-order polynomial model with regularization\n",
    "# model4.fit(x_train, y_train)\n",
    "# y_pred4 = model4.predict(x_test)\n",
    "# mse4 = mean_squared_error(y_test, y_pred4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b03d858d39980fb695ac11c435169bfe4a8bf74194aab29b7a5025b4dd6e7e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
