{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. As in the logistic regression tut, we will generate our own data to train a neural network. This time, we will generate data in the region [0, 1]^2 with a complicated decision boundary. In python, perform the following tasks:\n",
    "\n",
    "(a)\tDefine the function f (x) = x^2 sin(2πx) + 0.7.\n",
    "\n",
    "(b)\tGenerate a uniform random point (x1, x2) = [0, 1]^2. Associate with this point the class 0 if f (x1) > x2, and class 1 otherwise.\n",
    "\n",
    "(c)\tGenerate 100 points in this way. Plot them with different symbols for the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#function f(x)\n",
    "def f(x):\n",
    "    return x**2 * np.sin(2*np.pi*x) + 0.7\n",
    "\n",
    "#generate uniform random points and assign classes\n",
    "n_points = 100\n",
    "x1 = np.random.uniform(0, 1, n_points)\n",
    "x2 = np.random.uniform(0, 1, n_points)\n",
    "y = (f(x1) > x2).astype(int)\n",
    "points = np.column_stack((x1, x2, y))\n",
    "\n",
    "#plot the two classes\n",
    "class0 = points[points[:, 2] == 0]\n",
    "class1 = points[points[:, 2] == 1]\n",
    "plt.scatter(class0[:, 0], class0[:, 1], color='red', label='Class 0')\n",
    "plt.scatter(class1[:, 0], class1[:, 1], color='blue', label='Class 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For this question, use the dataset you generated for question 2 above. We are now going to train a neural network model to classify this data.\n",
    "\n",
    "(a)\tFirst we need to choose an architecture for the network. This data is 2D, in x1 and x2. We therefore need two input parameters for the model, and one output variable. It is a classification problem, so we can choose the final activation function to be a sigmoid. We know the decision boundary is non-linear because we made the data – otherwise we may need to visualise some of it to figure this out, and so we need at least one hidden layer. Let’s use three nodes on the hidden layer, and sigmoids for all activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "\n",
    "# Initialize the weights randomly\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "\n",
    "# Define the activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)\tImplement forward propagation for this network. Do this is a vectorised way, so we can generalise to different architectures. For any input vector, we need a vector of activations at every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "def forward(X):\n",
    "    # Calculate the dot product of the input layer with the weights of the hidden layer\n",
    "    hidden_layer = sigmoid(np.dot(X, W1))\n",
    "    # Calculate the dot product of the hidden layer with the weights of the output layer\n",
    "    output_layer = sigmoid(np.dot(hidden_layer, W2))\n",
    "    return output_layer, hidden_layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)\tNow compute the error δ at the final layer as the difference between the final activation and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the error\n",
    "def compute_error(output_layer, y):\n",
    "    error = y - output_layer\n",
    "    return error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)\tMoving backwards through the layers, compute the δ of each layer (in this case this would just be for the hidden layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the delta of the hidden layer\n",
    "def compute_delta_hidden(error, hidden_layer):\n",
    "    delta_hidden = np.dot(error, W2.T) * hidden_layer * (1 - hidden_layer)\n",
    "    return delta_hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e)\tGiven the activations and δ's, compute the gradients for each parameter. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Finally, perform a weight update. Use the learning rate of α = 0.1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g)\tRepeat the update in a loop. Technically there are two nested loops: for each epoch (iteration of the outer loop) we run through all our data points and update the weights. We then usually use two terminating conditions on the outer loop: the first is to look at the normed difference between the parameter vector between two successive iterations and we terminate when this is small, i.e. ||θnew - θold|| < ϵ, with ϵ = 0.05. We also usually set a maximum number of epochs, e.g. 1000, just in case. Run the learning until convergence. What is the error on the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 2 and the array at index 1 has size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mi:\\.COMS3007A - Machine Learning\\Tutorials\\Machine-Learning-Tutorials\\Machine-Learning-Tutorials\\Neural-Networks-Tutorial-2.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/.COMS3007A%20-%20Machine%20Learning/Tutorials/Machine-Learning-Tutorials/Machine-Learning-Tutorials/Neural-Networks-Tutorial-2.ipynb#X25sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m W1 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m dW1\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/.COMS3007A%20-%20Machine%20Learning/Tutorials/Machine-Learning-Tutorials/Machine-Learning-Tutorials/Neural-Networks-Tutorial-2.ipynb#X25sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Compute the normed difference\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/i%3A/.COMS3007A%20-%20Machine%20Learning/Tutorials/Machine-Learning-Tutorials/Machine-Learning-Tutorials/Neural-Networks-Tutorial-2.ipynb#X25sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m normed_difference \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(np\u001b[39m.\u001b[39;49mhstack((dW1, dW2)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/.COMS3007A%20-%20Machine%20Learning/Tutorials/Machine-Learning-Tutorials/Machine-Learning-Tutorials/Neural-Networks-Tutorial-2.ipynb#X25sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Print the error and the normed difference\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/.COMS3007A%20-%20Machine%20Learning/Tutorials/Machine-Learning-Tutorials/Machine-Learning-Tutorials/Neural-Networks-Tutorial-2.ipynb#X25sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, error \u001b[39m\u001b[39m{\u001b[39;00mtotal_error[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, normed difference \u001b[39m\u001b[39m{\u001b[39;00mnormed_difference\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\USER-PC\\Anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py:345\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39mconcatenate(arrs, \u001b[39m0\u001b[39m)\n\u001b[0;32m    344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 345\u001b[0m     \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 2 and the array at index 1 has size 3"
     ]
    }
   ],
   "source": [
    "# Set the learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Set the maximum number of epochs and the convergence threshold\n",
    "max_epochs = 1000\n",
    "convergence_threshold = 0.05\n",
    "\n",
    "# Initialize the epoch counter and the normed difference\n",
    "epoch = 0\n",
    "normed_difference = 1\n",
    "\n",
    "# Get the input data and target\n",
    "X = points[:, :2]\n",
    "y = points[:, 2].reshape(-1, 1)\n",
    "\n",
    "# Train the neural network\n",
    "while epoch < max_epochs and normed_difference > convergence_threshold:\n",
    "    # Initialize the gradients\n",
    "    dW1 = np.zeros_like(W1)\n",
    "    dW2 = np.zeros_like(W2)\n",
    "    # Initialize the total error\n",
    "    total_error = 0\n",
    "    # Iterate over all data points\n",
    "    for i in range(len(X)):\n",
    "        # Forward propagation\n",
    "        output_layer, hidden_layer = forward(X[i].reshape(1, -1))\n",
    "        # Compute the error\n",
    "        error = compute_error(output_layer, y[i])\n",
    "        # Compute the delta of the output layer\n",
    "        delta_output = error * output_layer * (1 - output_layer)\n",
    "        # Compute the delta of the hidden layer\n",
    "        delta_hidden = compute_delta_hidden(delta_output, hidden_layer)\n",
    "        # Compute the gradients\n",
    "        dW2 += np.dot(hidden_layer.T, delta_output)\n",
    "        dW1 += np.dot(X[i].reshape(1, -1).T, delta_hidden)\n",
    "        # Update the total error\n",
    "        total_error += np.abs(error)\n",
    "    # Update the weights\n",
    "    W2 += learning_rate * dW2\n",
    "    W1 += learning_rate * dW1\n",
    "    # Compute the normed difference\n",
    "    normed_difference = np.linalg.norm(np.hstack((dW1, dW2)))\n",
    "    # Print the error and the normed difference\n",
    "    print(f\"Epoch {epoch}, error {total_error[0][0]}, normed difference {normed_difference}\")\n",
    "    # Increment the epoch counter\n",
    "    epoch += 1\n",
    "\n",
    "# Compute the output of the neural network for all data points\n",
    "output, _ = forward(X)\n",
    "\n",
    "# Compute the error on the training data\n",
    "error = np.abs(y - output).mean()\n",
    "print(f\"Error on the training data: {error}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(h)\tGenerate 100 more datapoints from the procedure in question 2. This will be our validation data. Classify them using your trained model and tabulate the results in a confusion matrix. Compare the error on the training data to the error on the validation data. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate validation data\n",
    "n_val_points = 100\n",
    "x1_val = np.random.uniform(0, 1, n_val_points)\n",
    "x2_val = np.random.uniform(0, 1, n_val_points)\n",
    "y_val = (f(x1_val) > x2_val).astype(int)\n",
    "points_val = np.column_stack((x1_val, x2_val, y_val))\n",
    "\n",
    "# Compute predictions on validation data\n",
    "X_val = points_val[:, :2]\n",
    "y_val_pred = forward(X_val, weights)[1][-1] > 0.5\n",
    "y_val_true = points_val[:, -1]\n",
    "\n",
    "# Compute confusion matrix on validation data\n",
    "confusion_matrix_val = confusion_matrix(y_val_true, y_val_pred)\n",
    "print(\"Confusion matrix on validation data:\")\n",
    "print(confusion_matrix_val)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i)\tChange the hyperparameters. Before, we considered changing the learning rate α and termination threshold ϵ. Now try add another node or two to the hidden layer, and even add in a second hidden layer. For each different setting of the hyperparameters, retrain your model on the training data and evaluate it on the validation data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(j)\tKeep the best values of your hyperparameters. Now generate 100 more datapoints. This will be our testing data. Classify them using your trained model and tabulate the results in a confusion matrix. This is the final performance of the classifier with optimised hyperparameters!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(k)\tWhy is it important to have the three data sets: training, validation, and testing?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
