{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bonus question - for bonus marks* We will now look at a more challenging text-based classification\n",
    "problem, namely to classify a page from a Harry Potter book into which of the seven books the page was\n",
    "taken from. The books can be found in the zip file hp_books.zip and are text files where each page of a\n",
    "given book is a line in the text file. Note, all punctuation and capital letters have been removed from the\n",
    "file, so that only the words of the page remain to be used by our model.\n",
    "\n",
    "(a) Train an NB model using 80% of the data to train and the remaining 20% as test data. Use Laplace\n",
    "smoothing for your model. Report a confusion matrix of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[  5   0   2   8  36  11   8]\n",
      " [  0  12   1   9  20  19  15]\n",
      " [  0   0  35   2  44   4  13]\n",
      " [  0   0   0  70  36  16  40]\n",
      " [  0   0   0   4 152  16  49]\n",
      " [  0   0   0   5  28  55  58]\n",
      " [  1   0   1   4  31  17 116]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "\n",
    "#Read data from text files\n",
    "data_dir = \"hp_books\"\n",
    "book_pages = defaultdict(list)\n",
    "for book_file in os.listdir(data_dir):\n",
    "    with open(os.path.join(data_dir, book_file), \"r\") as f:\n",
    "        for line in f:\n",
    "            book_pages[book_file].append(line.strip())\n",
    "\n",
    "#Splitting data into sets\n",
    "train_pages = []\n",
    "test_pages = []\n",
    "for book in book_pages:\n",
    "    num_pages = len(book_pages[book])\n",
    "    split_index = int(num_pages * 0.8) # Use 80% of data for training\n",
    "    train_pages += [(page, book) for page in book_pages[book][:split_index]]\n",
    "    test_pages += [(page, book) for page in book_pages[book][split_index:]]\n",
    "\n",
    "vocabulary = set()\n",
    "class_counts = defaultdict(int)\n",
    "for page, book in train_pages:\n",
    "    class_counts[book] += 1\n",
    "    vocabulary.update(page.split())\n",
    "num_classes = len(class_counts)\n",
    "class_priors = {book: count/len(train_pages) for book, count in class_counts.items()}\n",
    "\n",
    "#Compute word counts for each class\n",
    "word_counts = {book: defaultdict(int) for book in class_counts}\n",
    "for page, book in train_pages:\n",
    "    for word in page.split():\n",
    "        word_counts[book][word] += 1\n",
    "\n",
    "#Training\n",
    "k = 1 \n",
    "class_likelihoods = {book: defaultdict(float) for book in class_counts}\n",
    "for book in word_counts:\n",
    "    total_words = sum(word_counts[book].values())\n",
    "    for word in vocabulary:\n",
    "        count = word_counts[book][word]\n",
    "        class_likelihoods[book][word] = (count + k) / (total_words + k * len(vocabulary))\n",
    "\n",
    "y_true = [book for _, book in test_pages]\n",
    "y_pred = []\n",
    "for page, _ in test_pages:\n",
    "    log_probs = {book: 0 for book in class_counts}\n",
    "    for book in class_counts:\n",
    "        log_probs[book] += math.log(class_priors[book])\n",
    "        for word in page.split():\n",
    "            if word in vocabulary:\n",
    "                log_probs[book] += math.log(class_likelihoods[book][word])\n",
    "    pred_class = max(log_probs, key=log_probs.get)\n",
    "    y_pred.append(pred_class)\n",
    "\n",
    "#Confusion Matrix\n",
    "labels = list(class_counts.keys())\n",
    "conf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "print(f\"Confusion matrix:\\n{conf_mat}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Adapt your code to use 80% of the data to train, 10% of the data as validation data and the remaining\n",
    "10% as test data. Train separate NB classifiers using the values {1  * 10^-1; 1  * 10^-2; 1  * 10^-3; 1  * 10^-4; 1 * 10^-5; 1  * 10^-6} to smooth the table of likelihoods. Train each model using the training data,\n",
    "and track its performance on the validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[ 7  1  4  4 17  2  0]\n",
      " [ 0  9  2  3 14  3  7]\n",
      " [ 0  0 24  1  9  3 12]\n",
      " [ 0  0  0 31 19  8 23]\n",
      " [ 0  0  0  4 96  1  9]\n",
      " [ 0  0  0  7 18 26 22]\n",
      " [ 0  1  2  3 16  9 54]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "\n",
    "# Read data from text files\n",
    "data_dir = \"hp_books\"\n",
    "book_pages = defaultdict(list)\n",
    "for book_file in os.listdir(data_dir):\n",
    "    with open(os.path.join(data_dir, book_file), \"r\") as f:\n",
    "        for line in f:\n",
    "            book_pages[book_file].append(line.strip())\n",
    "\n",
    "train_pages = []\n",
    "val_pages = []\n",
    "test_pages = []\n",
    "for book in book_pages:\n",
    "    num_pages = len(book_pages[book])\n",
    "    train_split_index = int(num_pages * 0.8)  # Use 80% of data for training\n",
    "    val_split_index = int(num_pages * 0.9)  # Use 10% of data for validation\n",
    "    train_pages += [(page, book) for page in book_pages[book][:train_split_index]]\n",
    "    val_pages += [(page, book) for page in book_pages[book][train_split_index:val_split_index]]\n",
    "    test_pages += [(page, book) for page in book_pages[book][val_split_index:]]\n",
    "\n",
    "vocabulary = set()\n",
    "class_counts = defaultdict(int)\n",
    "for page, book in train_pages:\n",
    "    class_counts[book] += 1\n",
    "    vocabulary.update(page.split())\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "total_train_pages = len(train_pages)\n",
    "class_priors = {book: count / total_train_pages for book, count in class_counts.items()}\n",
    "smoothing_vals = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "best_accuracy = 0\n",
    "best_conf_mat = None\n",
    "for k in smoothing_vals:\n",
    "    word_counts = {book: defaultdict(int) for book in class_counts}\n",
    "    for page, book in train_pages:\n",
    "        for word in page.split():\n",
    "            word_counts[book][word] += 1\n",
    "\n",
    "    class_likelihoods = {book: defaultdict(float) for book in class_counts}\n",
    "    for book in word_counts:\n",
    "        total_words = sum(word_counts[book].values())\n",
    "        for word in vocabulary:\n",
    "            count = word_counts[book][word]\n",
    "            class_likelihoods[book][word] = (count + k) / (total_words + k * len(vocabulary))\n",
    "\n",
    "    y_true = [book for _, book in val_pages]\n",
    "    y_pred = []\n",
    "    for page, _ in val_pages:\n",
    "        log_probs = {book: 0 for book in class_counts}\n",
    "        for book in class_counts:\n",
    "            log_probs[book] += math.log(class_priors[book])\n",
    "            for word in page.split():\n",
    "                if word in vocabulary:\n",
    "                    log_probs[book] += math.log(class_likelihoods[book][word])\n",
    "        pred_class = max(log_probs, key=log_probs.get)\n",
    "        y_pred.append(pred_class)\n",
    "\n",
    "    accuracy = sum([1 for i in range(len(y_true)) if y_true[i] == y_pred[i]]) / len(y_true)\n",
    "    labels = list(class_counts.keys())\n",
    "    conf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "print(f\"Confusion matrix:\\n{conf_mat}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
