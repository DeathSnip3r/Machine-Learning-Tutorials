{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bonus question - for bonus marks* We will now look at a more challenging text-based classification\n",
    "problem, namely to classify a page from a Harry Potter book into which of the seven books the page was\n",
    "taken from. The books can be found in the zip file hp_books.zip and are text files where each page of a\n",
    "given book is a line in the text file. Note, all punctuation and capital letters have been removed from the\n",
    "file, so that only the words of the page remain to be used by our model.\n",
    "\n",
    "(a) Train an NB model using 80% of the data to train and the remaining 20% as test data. Use Laplace\n",
    "smoothing for your model. Report a confusion matrix of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[  5   0   2   8  36  11   8]\n",
      " [  0  12   1   9  20  19  15]\n",
      " [  0   0  35   2  44   4  13]\n",
      " [  0   0   0  70  36  16  40]\n",
      " [  0   0   0   4 152  16  49]\n",
      " [  0   0   0   5  28  55  58]\n",
      " [  1   0   1   4  31  17 116]]\n",
      "Accuracy: 47.1898197242842%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "\n",
    "#Read data from text files\n",
    "data_dir = \"hp_books\"\n",
    "book_pages = defaultdict(list)\n",
    "for book_file in os.listdir(data_dir):\n",
    "    with open(os.path.join(data_dir, book_file), \"r\") as f:\n",
    "        for line in f:\n",
    "            book_pages[book_file].append(line.strip())\n",
    "\n",
    "#Splitting data into sets\n",
    "train_pages = []\n",
    "test_pages = []\n",
    "for book in book_pages:\n",
    "    num_pages = len(book_pages[book])\n",
    "    split_index = int(num_pages * 0.8) # Use 80% of data for training\n",
    "    train_pages += [(page, book) for page in book_pages[book][:split_index]]\n",
    "    test_pages += [(page, book) for page in book_pages[book][split_index:]]\n",
    "\n",
    "vocabulary = set()\n",
    "class_counts = defaultdict(int)\n",
    "for page, book in train_pages:\n",
    "    class_counts[book] += 1\n",
    "    vocabulary.update(page.split())\n",
    "num_classes = len(class_counts)\n",
    "class_priors = {book: count/len(train_pages) for book, count in class_counts.items()}\n",
    "\n",
    "#Compute word counts for each class\n",
    "word_counts = {book: defaultdict(int) for book in class_counts}\n",
    "for page, book in train_pages:\n",
    "    for word in page.split():\n",
    "        word_counts[book][word] += 1\n",
    "\n",
    "#Training\n",
    "k = 1 \n",
    "class_likelihoods = {book: defaultdict(float) for book in class_counts}\n",
    "for book in word_counts:\n",
    "    total_words = sum(word_counts[book].values())\n",
    "    for word in vocabulary:\n",
    "        count = word_counts[book][word]\n",
    "        class_likelihoods[book][word] = (count + k) / (total_words + k * len(vocabulary))\n",
    "\n",
    "y_true = [book for _, book in test_pages]\n",
    "y_pred = []\n",
    "for page, _ in test_pages:\n",
    "    log_probs = {book: 0 for book in class_counts}\n",
    "    for book in class_counts:\n",
    "        log_probs[book] += math.log(class_priors[book])\n",
    "        for word in page.split():\n",
    "            if word in vocabulary:\n",
    "                log_probs[book] += math.log(class_likelihoods[book][word])\n",
    "    pred_class = max(log_probs, key=log_probs.get)\n",
    "    y_pred.append(pred_class)\n",
    "\n",
    "#Confusion Matrix\n",
    "labels = list(class_counts.keys())\n",
    "print_confusion_matrix = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "print(f\"Confusion matrix:\\n{print_confusion_matrix}\")\n",
    "#not required\n",
    "accuracy = sum([1 for i in range(len(y_true)) if y_true[i] == y_pred[i]]) / len(y_true)\n",
    "print(f\"Accuracy: {round(accuracy*100, 4)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Adapt your code to use 80% of the data to train, 10% of the data as validation data and the remaining\n",
    "10% as test data. Train separate NB classifiers using the values {1  * 10^-1; 1  * 10^-2; 1  * 10^-3; 1  * 10^-4; 1 * 10^-5; 1  * 10^-6} to smooth the table of likelihoods. Train each model using the training data,\n",
    "and track its performance on the validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[ 7  1  4  4 17  2  0]\n",
      " [ 0  9  2  3 14  3  7]\n",
      " [ 0  0 24  1  9  3 12]\n",
      " [ 0  0  0 31 19  8 23]\n",
      " [ 0  0  0  4 96  1  9]\n",
      " [ 0  0  0  7 18 26 22]\n",
      " [ 0  1  2  3 16  9 54]]\n",
      "Smoothing values: [0.1, 0.01, 0.001, 0.0001, 1e-05, 1e-06]\n",
      "Accuracy on Validation Performance: ['65.1805%', '63.4820%', '59.2357%', '56.2633%', '53.9278%', '52.4416%']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "\n",
    "# Read data from text files\n",
    "data_dir = \"hp_books\"\n",
    "book_pages = defaultdict(list)\n",
    "for book_file in os.listdir(data_dir):\n",
    "    with open(os.path.join(data_dir, book_file), \"r\") as f:\n",
    "        for line in f:\n",
    "            book_pages[book_file].append(line.strip())\n",
    "\n",
    "train_pages = []\n",
    "val_pages = []\n",
    "test_pages = []\n",
    "for book in book_pages:\n",
    "    num_pages = len(book_pages[book])\n",
    "    train_split_index = int(num_pages * 0.8)  # Use 80% of data for training\n",
    "    val_split_index = int(num_pages * 0.9)  # Use 10% of data for validation\n",
    "    train_pages += [(page, book) for page in book_pages[book][:train_split_index]]\n",
    "    val_pages += [(page, book) for page in book_pages[book][train_split_index:val_split_index]]\n",
    "    test_pages += [(page, book) for page in book_pages[book][val_split_index:]]\n",
    "\n",
    "vocabulary = set()\n",
    "class_counts = defaultdict(int)\n",
    "for page, book in train_pages:\n",
    "    class_counts[book] += 1\n",
    "    vocabulary.update(page.split())\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "total_train_pages = len(train_pages)\n",
    "class_priors = {book: count / total_train_pages for book, count in class_counts.items()}\n",
    "smoothing_vals = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "best_accuracy = 0\n",
    "best_conf_mat = None\n",
    "#track performance on validation data\n",
    "val_performance = []\n",
    "for k in smoothing_vals:\n",
    "    word_counts = {book: defaultdict(int) for book in class_counts}\n",
    "    for page, book in train_pages:\n",
    "        for word in page.split():\n",
    "            word_counts[book][word] += 1\n",
    "\n",
    "    class_likelihoods = {book: defaultdict(float) for book in class_counts}\n",
    "    for book in word_counts:\n",
    "        total_words = sum(word_counts[book].values())\n",
    "        for word in vocabulary:\n",
    "            count = word_counts[book][word]\n",
    "            class_likelihoods[book][word] = (count + k) / (total_words + k * len(vocabulary))\n",
    "\n",
    "    y_true = [book for _, book in val_pages]\n",
    "    y_pred = []\n",
    "    for page, _ in val_pages:\n",
    "        log_probs = {book: 0 for book in class_counts}\n",
    "        for book in class_counts:\n",
    "            log_probs[book] += math.log(class_priors[book])\n",
    "            for word in page.split():\n",
    "                if word in vocabulary:\n",
    "                    log_probs[book] += math.log(class_likelihoods[book][word])\n",
    "        pred_class = max(log_probs, key=log_probs.get)\n",
    "        y_pred.append(pred_class)\n",
    "\n",
    "    accuracy = round((sum([1 for i in range(len(y_true)) if y_true[i] == y_pred[i]]) / len(y_true)), 8)\n",
    "    val_performance.append(accuracy)\n",
    "    labels = list(class_counts.keys())\n",
    "    print_confusion_matrix = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "for i in range(len(val_performance)):\n",
    "    val_performance[i] = \"{:.4f}%\".format(val_performance[i] * 100)\n",
    "print(f\"Confusion matrix:\\n{print_confusion_matrix}\")\n",
    "print(f\"Smoothing values: {smoothing_vals}\\nAccuracy on Validation Performance: {val_performance}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Use the model which achieved the best validation accuracy and test it using the test data set. Report a confusion matrix of the results, as well as the test accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[19  1  2  3  4  6  0]\n",
      " [ 0 19  0  1  2 13  3]\n",
      " [ 0  0 29  2 13  2  3]\n",
      " [ 1  0  0 40 17  9 14]\n",
      " [ 0  1  1  2 44 23 40]\n",
      " [ 0  1  1  1 14 24 32]\n",
      " [ 1  1  0  0  6 14 63]]\n",
      "Test Accuracy for 1e-1: 0.5042 = 50.4237%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "\n",
    "#Read data from text files\n",
    "data_dir = \"hp_books\"\n",
    "book_pages = defaultdict(list)\n",
    "for book_file in os.listdir(data_dir):\n",
    "    with open(os.path.join(data_dir, book_file), \"r\") as f:\n",
    "        for line in f:\n",
    "            book_pages[book_file].append(line.strip())\n",
    "\n",
    "train_pages = []\n",
    "val_pages = []\n",
    "test_pages = []\n",
    "for book in book_pages:\n",
    "    num_pages = len(book_pages[book])\n",
    "    train_split_index = int(num_pages * 0.8)  #Use 80% of data for training\n",
    "    val_split_index = int(num_pages * 0.9)  #Use 10% of data for validation, just doing this based off the previous question, dont think we need to do 80% training and 20% test data\n",
    "    train_pages += [(page, book) for page in book_pages[book][:train_split_index]]\n",
    "    val_pages += [(page, book) for page in book_pages[book][train_split_index:val_split_index]]\n",
    "    test_pages += [(page, book) for page in book_pages[book][val_split_index:]]\n",
    "\n",
    "vocabulary = set()\n",
    "class_counts = defaultdict(int)\n",
    "for page, book in train_pages:\n",
    "    class_counts[book] += 1\n",
    "    vocabulary.update(page.split())\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "total_train_pages = len(train_pages)\n",
    "class_priors = {book: count / total_train_pages for book, count in class_counts.items()}\n",
    "\n",
    "#Train using the (1*10^-1) smoothing value\n",
    "k = 1e-1\n",
    "word_counts = {book: defaultdict(int) for book in class_counts}\n",
    "for page, book in train_pages:\n",
    "    for word in page.split():\n",
    "        word_counts[book][word] += 1\n",
    "\n",
    "class_likelihoods = {book: defaultdict(float) for book in class_counts}\n",
    "for book in word_counts:\n",
    "    total_words = sum(word_counts[book].values())\n",
    "    for word in vocabulary:\n",
    "        count = word_counts[book][word]\n",
    "        class_likelihoods[book][word] = (count + k) / (total_words + k * len(vocabulary))\n",
    "\n",
    "y_true = [book for _, book in test_pages]\n",
    "y_pred = []\n",
    "for page, _ in test_pages:\n",
    "    log_probs = {book: 0 for book in class_counts}\n",
    "    for book in class_counts:\n",
    "        log_probs[book] += math.log(class_priors[book])\n",
    "        for word in page.split():\n",
    "            if word in vocabulary:\n",
    "                log_probs[book] += math.log(class_likelihoods[book][word])\n",
    "    pred_class = max(log_probs, key=log_probs.get)\n",
    "    y_pred.append(pred_class)\n",
    "\n",
    "accuracy = sum([1 for i in range(len(y_true)) if y_true[i] == y_pred[i]]) / len(y_true)\n",
    "print_confusion_matrix = confusion_matrix(y_true, y_pred, labels=list(class_counts.keys()))\n",
    "print(f\"Confusion matrix:\\n{print_confusion_matrix}\")\n",
    "print(f\"Test Accuracy for 1e-1: {round(accuracy,4)} = {round(accuracy*100, 4)}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
